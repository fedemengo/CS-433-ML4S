{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b9ea2757-1f2e-4c92-b071-3b8aeccf24ee",
   "metadata": {},
   "source": [
    "# plan\n",
    "\n",
    "- models with `forward`\n",
    "- model specfic trainer to handle the training step\n",
    "- model selection through hyperparameters optimization AND data augmentation: since 1 strategy of data augmentation might work better for a specific model it makes sense to optimize both jointly\n",
    "- `./models/selection.py` should return actual best model and comparison results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "04105407-2dbf-4823-8d19-3ec0e2664afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "import xarray as xr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dad07e01-f241-471d-b439-dd29cd1c907f",
   "metadata": {},
   "source": [
    "## models definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "48cdcc28-6a0b-4f4b-a294-a16ced1d51e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNModel(nn.Module):\n",
    "   def __init__(self, input_size, hidden_size, output_size):\n",
    "       super(RNNModel, self).__init__()\n",
    "       self.rnn = nn.LSTM(input_size, hidden_size, batch_first=True, num_layers=2) # Either GRU or LSTM\n",
    "       self.fc = nn.Linear(hidden_size, output_size)  # Maps hidden state to output\n",
    "\n",
    "   def forward(self, x):\n",
    "       # Pass through RNN\n",
    "       rnn_out, _ = self.rnn(x)  # rnn_out: (batch_size, seq_len, hidden_size)\n",
    "       \n",
    "       # Apply fully connected layer\n",
    "       output = self.fc(rnn_out)  # output: (batch_size, seq_len, output_size)\n",
    "       return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "105d284f-457a-4e14-a890-98db5a9a3541",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepRNNModel(nn.Module):\n",
    "   def __init__(self, input_size, hidden_size, output_size, num_layers=3, dropout_prob=0.2):\n",
    "       super(RNNModel, self).__init__()\n",
    "       \n",
    "       # Stack RNN layers\n",
    "       self.rnn = nn.RNN(input_size, hidden_size, num_layers=num_layers, batch_first=True, dropout=dropout_prob)\n",
    "       self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "   def forward(self, x):\n",
    "       # Pass through stacked RNN layers\n",
    "       rnn_out, _ = self.rnn(x)  # rnn_out: (batch_size, seq_len, hidden_size)\n",
    "       \n",
    "       # Apply fully connected layer to the last time step's output\n",
    "       output = self.fc(rnn_out)  # Use only the last time step for prediction\n",
    "       return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2ddf1174-caf8-4bc7-abc1-24da4ac289fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionRNNModel(nn.Module):\n",
    "   def __init__(self, input_size, hidden_size, output_size, dropout_prob=0.2):\n",
    "       super(RNNModel, self).__init__()\n",
    "       \n",
    "       self.lstm = nn.LSTM(input_size, hidden_size, batch_first=True, num_layers=2, dropout=dropout_prob)\n",
    "       self.attn = nn.Linear(hidden_size, hidden_size)\n",
    "       self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "   def forward(self, x):\n",
    "       # Pass through LSTM\n",
    "       lstm_out, _ = self.lstm(x)  # lstm_out: (batch_size, seq_len, hidden_size)\n",
    "       \n",
    "       # Apply attention mechanism at each time step\n",
    "       attn_weights = F.softmax(self.attn(lstm_out), dim=2)  # Shape: (batch_size, seq_len, hidden_size)\n",
    "       weighted_out = attn_weights * lstm_out  # Apply attention: (batch_size, seq_len, hidden_size)\n",
    "       \n",
    "       # Pass weighted outputs through fully connected layer\n",
    "       output = self.fc(weighted_out)  # Shape: (batch_size, seq_len, output_size)\n",
    "       return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9a78e92a-6f13-4184-81e2-ff20b6d3413e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRUModel(nn.Module):\n",
    "   def __init__(self, input_size, hidden_size, output_size, dropout_prob=0.2):\n",
    "       super(RNNModel, self).__init__()\n",
    "       \n",
    "       # GRU layer\n",
    "       self.gru = nn.GRU(input_size, hidden_size, batch_first=True, num_layers=2, dropout=dropout_prob)\n",
    "       # Fully connected layer\n",
    "       self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "   def forward(self, x):\n",
    "       # Pass through GRU\n",
    "       gru_out, _ = self.gru(x)  # gru_out: (batch_size, seq_len, hidden_size)\n",
    "       \n",
    "       # Apply fully connected layer to the last time step's output\n",
    "       output = self.fc(gru_out)  # Use only the last time step for prediction\n",
    "       return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0a1fb07-b85f-4ebf-84cb-4126b01258b5",
   "metadata": {},
   "source": [
    "## specific trainer per model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "81f1990b-1de7-42ed-a57a-32b61554a3af",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseTrainer:\n",
    "    def train_step(self, batch):\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d85e0024-831a-4733-8972-2bda1caadcbe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8486feb6-bd15-4865-9ff8-0702f9c41c26",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ede46775-9473-478d-be11-19d9e00d574a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "11.6\n",
      "True\n",
      "4\n",
      "1.13.1\n",
      "cuda\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.is_available())\n",
    "print(torch.version.cuda)\n",
    "print(torch.backends.cudnn.enabled)\n",
    "print(torch.cuda.device_count())\n",
    "print(torch.__version__)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "28ceeeff-2a66-4976-9dce-48bd0119eb0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AugmentationStrategies:\n",
    "   @staticmethod\n",
    "   def add_gaussian_noise(data, intensity=0.01, probability=1.0):\n",
    "       augmented = data.copy()\n",
    "       mask = np.random.random(len(data)) < probability\n",
    "       noise = np.random.normal(0, intensity, size=data[mask].shape)\n",
    "       augmented[mask] = augmented[mask] + noise\n",
    "       return augmented\n",
    "\n",
    "   @staticmethod\n",
    "   def scaling_augmentation(data, min_scale=0.8, max_scale=1.2):\n",
    "       augmented = data.copy()\n",
    "       scales = np.random.uniform(min_scale, max_scale, size=(len(data), 1, data.shape[2]))\n",
    "       augmented = augmented * scales\n",
    "       return augmented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "07ee087b-bc93-4436-ad6e-fcd63908eef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, dropout_prob=0.2):\n",
    "        super(RNNModel, self).__init__()\n",
    "        \n",
    "        # Bidirectional LSTM layer\n",
    "        self.bilstm = nn.LSTM(input_size, hidden_size, batch_first=True, num_layers=2, bidirectional=True, dropout=dropout_prob)\n",
    "        # Fully connected layer\n",
    "        self.fc = nn.Linear(hidden_size * 2, output_size)  # *2 because of bidirectional\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Pass through BiLSTM\n",
    "        lstm_out, _ = self.bilstm(x)  # lstm_out: (batch_size, seq_len, hidden_size * 2)\n",
    "        \n",
    "        # Use the output from the last time step\n",
    "        output = self.fc(lstm_out)  # Last time step output\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "332824f6-ccf5-4738-a5cb-0d994e491922",
   "metadata": {},
   "source": [
    "## data preparation\n",
    "- first split between train and test\n",
    "  - `random_subject_split` by ratio\n",
    "  - `k_fold`\n",
    "- second split between train and validation\n",
    "  - `k_fold`\n",
    "  - `LOO`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e6e09043-5fdc-4193-8239-7324baf17b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "nc_file = 'data/derivatives/dataset_MOTOR_60_subjects_both.nc' # hrf convoluted + block, if we want to denoise data\n",
    "np.random.seed(459345)\n",
    "dataset = xr.open_dataset(nc_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fc217756-21ed-4449-88d7-961d5777aee8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><svg style=\"position: absolute; width: 0; height: 0; overflow: hidden\">\n",
       "<defs>\n",
       "<symbol id=\"icon-database\" viewBox=\"0 0 32 32\">\n",
       "<path d=\"M16 0c-8.837 0-16 2.239-16 5v4c0 2.761 7.163 5 16 5s16-2.239 16-5v-4c0-2.761-7.163-5-16-5z\"></path>\n",
       "<path d=\"M16 17c-8.837 0-16-2.239-16-5v6c0 2.761 7.163 5 16 5s16-2.239 16-5v-6c0 2.761-7.163 5-16 5z\"></path>\n",
       "<path d=\"M16 26c-8.837 0-16-2.239-16-5v6c0 2.761 7.163 5 16 5s16-2.239 16-5v-6c0 2.761-7.163 5-16 5z\"></path>\n",
       "</symbol>\n",
       "<symbol id=\"icon-file-text2\" viewBox=\"0 0 32 32\">\n",
       "<path d=\"M28.681 7.159c-0.694-0.947-1.662-2.053-2.724-3.116s-2.169-2.030-3.116-2.724c-1.612-1.182-2.393-1.319-2.841-1.319h-15.5c-1.378 0-2.5 1.121-2.5 2.5v27c0 1.378 1.122 2.5 2.5 2.5h23c1.378 0 2.5-1.122 2.5-2.5v-19.5c0-0.448-0.137-1.23-1.319-2.841zM24.543 5.457c0.959 0.959 1.712 1.825 2.268 2.543h-4.811v-4.811c0.718 0.556 1.584 1.309 2.543 2.268zM28 29.5c0 0.271-0.229 0.5-0.5 0.5h-23c-0.271 0-0.5-0.229-0.5-0.5v-27c0-0.271 0.229-0.5 0.5-0.5 0 0 15.499-0 15.5 0v7c0 0.552 0.448 1 1 1h7v19.5z\"></path>\n",
       "<path d=\"M23 26h-14c-0.552 0-1-0.448-1-1s0.448-1 1-1h14c0.552 0 1 0.448 1 1s-0.448 1-1 1z\"></path>\n",
       "<path d=\"M23 22h-14c-0.552 0-1-0.448-1-1s0.448-1 1-1h14c0.552 0 1 0.448 1 1s-0.448 1-1 1z\"></path>\n",
       "<path d=\"M23 18h-14c-0.552 0-1-0.448-1-1s0.448-1 1-1h14c0.552 0 1 0.448 1 1s-0.448 1-1 1z\"></path>\n",
       "</symbol>\n",
       "</defs>\n",
       "</svg>\n",
       "<style>/* CSS stylesheet for displaying xarray objects in jupyterlab.\n",
       " *\n",
       " */\n",
       "\n",
       ":root {\n",
       "  --xr-font-color0: var(--jp-content-font-color0, rgba(0, 0, 0, 1));\n",
       "  --xr-font-color2: var(--jp-content-font-color2, rgba(0, 0, 0, 0.54));\n",
       "  --xr-font-color3: var(--jp-content-font-color3, rgba(0, 0, 0, 0.38));\n",
       "  --xr-border-color: var(--jp-border-color2, #e0e0e0);\n",
       "  --xr-disabled-color: var(--jp-layout-color3, #bdbdbd);\n",
       "  --xr-background-color: var(--jp-layout-color0, white);\n",
       "  --xr-background-color-row-even: var(--jp-layout-color1, white);\n",
       "  --xr-background-color-row-odd: var(--jp-layout-color2, #eeeeee);\n",
       "}\n",
       "\n",
       "html[theme=dark],\n",
       "body[data-theme=dark],\n",
       "body.vscode-dark {\n",
       "  --xr-font-color0: rgba(255, 255, 255, 1);\n",
       "  --xr-font-color2: rgba(255, 255, 255, 0.54);\n",
       "  --xr-font-color3: rgba(255, 255, 255, 0.38);\n",
       "  --xr-border-color: #1F1F1F;\n",
       "  --xr-disabled-color: #515151;\n",
       "  --xr-background-color: #111111;\n",
       "  --xr-background-color-row-even: #111111;\n",
       "  --xr-background-color-row-odd: #313131;\n",
       "}\n",
       "\n",
       ".xr-wrap {\n",
       "  display: block !important;\n",
       "  min-width: 300px;\n",
       "  max-width: 700px;\n",
       "}\n",
       "\n",
       ".xr-text-repr-fallback {\n",
       "  /* fallback to plain text repr when CSS is not injected (untrusted notebook) */\n",
       "  display: none;\n",
       "}\n",
       "\n",
       ".xr-header {\n",
       "  padding-top: 6px;\n",
       "  padding-bottom: 6px;\n",
       "  margin-bottom: 4px;\n",
       "  border-bottom: solid 1px var(--xr-border-color);\n",
       "}\n",
       "\n",
       ".xr-header > div,\n",
       ".xr-header > ul {\n",
       "  display: inline;\n",
       "  margin-top: 0;\n",
       "  margin-bottom: 0;\n",
       "}\n",
       "\n",
       ".xr-obj-type,\n",
       ".xr-array-name {\n",
       "  margin-left: 2px;\n",
       "  margin-right: 10px;\n",
       "}\n",
       "\n",
       ".xr-obj-type {\n",
       "  color: var(--xr-font-color2);\n",
       "}\n",
       "\n",
       ".xr-sections {\n",
       "  padding-left: 0 !important;\n",
       "  display: grid;\n",
       "  grid-template-columns: 150px auto auto 1fr 20px 20px;\n",
       "}\n",
       "\n",
       ".xr-section-item {\n",
       "  display: contents;\n",
       "}\n",
       "\n",
       ".xr-section-item input {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       ".xr-section-item input + label {\n",
       "  color: var(--xr-disabled-color);\n",
       "}\n",
       "\n",
       ".xr-section-item input:enabled + label {\n",
       "  cursor: pointer;\n",
       "  color: var(--xr-font-color2);\n",
       "}\n",
       "\n",
       ".xr-section-item input:enabled + label:hover {\n",
       "  color: var(--xr-font-color0);\n",
       "}\n",
       "\n",
       ".xr-section-summary {\n",
       "  grid-column: 1;\n",
       "  color: var(--xr-font-color2);\n",
       "  font-weight: 500;\n",
       "}\n",
       "\n",
       ".xr-section-summary > span {\n",
       "  display: inline-block;\n",
       "  padding-left: 0.5em;\n",
       "}\n",
       "\n",
       ".xr-section-summary-in:disabled + label {\n",
       "  color: var(--xr-font-color2);\n",
       "}\n",
       "\n",
       ".xr-section-summary-in + label:before {\n",
       "  display: inline-block;\n",
       "  content: '►';\n",
       "  font-size: 11px;\n",
       "  width: 15px;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       ".xr-section-summary-in:disabled + label:before {\n",
       "  color: var(--xr-disabled-color);\n",
       "}\n",
       "\n",
       ".xr-section-summary-in:checked + label:before {\n",
       "  content: '▼';\n",
       "}\n",
       "\n",
       ".xr-section-summary-in:checked + label > span {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       ".xr-section-summary,\n",
       ".xr-section-inline-details {\n",
       "  padding-top: 4px;\n",
       "  padding-bottom: 4px;\n",
       "}\n",
       "\n",
       ".xr-section-inline-details {\n",
       "  grid-column: 2 / -1;\n",
       "}\n",
       "\n",
       ".xr-section-details {\n",
       "  display: none;\n",
       "  grid-column: 1 / -1;\n",
       "  margin-bottom: 5px;\n",
       "}\n",
       "\n",
       ".xr-section-summary-in:checked ~ .xr-section-details {\n",
       "  display: contents;\n",
       "}\n",
       "\n",
       ".xr-array-wrap {\n",
       "  grid-column: 1 / -1;\n",
       "  display: grid;\n",
       "  grid-template-columns: 20px auto;\n",
       "}\n",
       "\n",
       ".xr-array-wrap > label {\n",
       "  grid-column: 1;\n",
       "  vertical-align: top;\n",
       "}\n",
       "\n",
       ".xr-preview {\n",
       "  color: var(--xr-font-color3);\n",
       "}\n",
       "\n",
       ".xr-array-preview,\n",
       ".xr-array-data {\n",
       "  padding: 0 5px !important;\n",
       "  grid-column: 2;\n",
       "}\n",
       "\n",
       ".xr-array-data,\n",
       ".xr-array-in:checked ~ .xr-array-preview {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       ".xr-array-in:checked ~ .xr-array-data,\n",
       ".xr-array-preview {\n",
       "  display: inline-block;\n",
       "}\n",
       "\n",
       ".xr-dim-list {\n",
       "  display: inline-block !important;\n",
       "  list-style: none;\n",
       "  padding: 0 !important;\n",
       "  margin: 0;\n",
       "}\n",
       "\n",
       ".xr-dim-list li {\n",
       "  display: inline-block;\n",
       "  padding: 0;\n",
       "  margin: 0;\n",
       "}\n",
       "\n",
       ".xr-dim-list:before {\n",
       "  content: '(';\n",
       "}\n",
       "\n",
       ".xr-dim-list:after {\n",
       "  content: ')';\n",
       "}\n",
       "\n",
       ".xr-dim-list li:not(:last-child):after {\n",
       "  content: ',';\n",
       "  padding-right: 5px;\n",
       "}\n",
       "\n",
       ".xr-has-index {\n",
       "  font-weight: bold;\n",
       "}\n",
       "\n",
       ".xr-var-list,\n",
       ".xr-var-item {\n",
       "  display: contents;\n",
       "}\n",
       "\n",
       ".xr-var-item > div,\n",
       ".xr-var-item label,\n",
       ".xr-var-item > .xr-var-name span {\n",
       "  background-color: var(--xr-background-color-row-even);\n",
       "  margin-bottom: 0;\n",
       "}\n",
       "\n",
       ".xr-var-item > .xr-var-name:hover span {\n",
       "  padding-right: 5px;\n",
       "}\n",
       "\n",
       ".xr-var-list > li:nth-child(odd) > div,\n",
       ".xr-var-list > li:nth-child(odd) > label,\n",
       ".xr-var-list > li:nth-child(odd) > .xr-var-name span {\n",
       "  background-color: var(--xr-background-color-row-odd);\n",
       "}\n",
       "\n",
       ".xr-var-name {\n",
       "  grid-column: 1;\n",
       "}\n",
       "\n",
       ".xr-var-dims {\n",
       "  grid-column: 2;\n",
       "}\n",
       "\n",
       ".xr-var-dtype {\n",
       "  grid-column: 3;\n",
       "  text-align: right;\n",
       "  color: var(--xr-font-color2);\n",
       "}\n",
       "\n",
       ".xr-var-preview {\n",
       "  grid-column: 4;\n",
       "}\n",
       "\n",
       ".xr-index-preview {\n",
       "  grid-column: 2 / 5;\n",
       "  color: var(--xr-font-color2);\n",
       "}\n",
       "\n",
       ".xr-var-name,\n",
       ".xr-var-dims,\n",
       ".xr-var-dtype,\n",
       ".xr-preview,\n",
       ".xr-attrs dt {\n",
       "  white-space: nowrap;\n",
       "  overflow: hidden;\n",
       "  text-overflow: ellipsis;\n",
       "  padding-right: 10px;\n",
       "}\n",
       "\n",
       ".xr-var-name:hover,\n",
       ".xr-var-dims:hover,\n",
       ".xr-var-dtype:hover,\n",
       ".xr-attrs dt:hover {\n",
       "  overflow: visible;\n",
       "  width: auto;\n",
       "  z-index: 1;\n",
       "}\n",
       "\n",
       ".xr-var-attrs,\n",
       ".xr-var-data,\n",
       ".xr-index-data {\n",
       "  display: none;\n",
       "  background-color: var(--xr-background-color) !important;\n",
       "  padding-bottom: 5px !important;\n",
       "}\n",
       "\n",
       ".xr-var-attrs-in:checked ~ .xr-var-attrs,\n",
       ".xr-var-data-in:checked ~ .xr-var-data,\n",
       ".xr-index-data-in:checked ~ .xr-index-data {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       ".xr-var-data > table {\n",
       "  float: right;\n",
       "}\n",
       "\n",
       ".xr-var-name span,\n",
       ".xr-var-data,\n",
       ".xr-index-name div,\n",
       ".xr-index-data,\n",
       ".xr-attrs {\n",
       "  padding-left: 25px !important;\n",
       "}\n",
       "\n",
       ".xr-attrs,\n",
       ".xr-var-attrs,\n",
       ".xr-var-data,\n",
       ".xr-index-data {\n",
       "  grid-column: 1 / -1;\n",
       "}\n",
       "\n",
       "dl.xr-attrs {\n",
       "  padding: 0;\n",
       "  margin: 0;\n",
       "  display: grid;\n",
       "  grid-template-columns: 125px auto;\n",
       "}\n",
       "\n",
       ".xr-attrs dt,\n",
       ".xr-attrs dd {\n",
       "  padding: 0;\n",
       "  margin: 0;\n",
       "  float: left;\n",
       "  padding-right: 10px;\n",
       "  width: auto;\n",
       "}\n",
       "\n",
       ".xr-attrs dt {\n",
       "  font-weight: normal;\n",
       "  grid-column: 1;\n",
       "}\n",
       "\n",
       ".xr-attrs dt:hover span {\n",
       "  display: inline-block;\n",
       "  background: var(--xr-background-color);\n",
       "  padding-right: 10px;\n",
       "}\n",
       "\n",
       ".xr-attrs dd {\n",
       "  grid-column: 2;\n",
       "  white-space: pre-wrap;\n",
       "  word-break: break-all;\n",
       "}\n",
       "\n",
       ".xr-icon-database,\n",
       ".xr-icon-file-text2,\n",
       ".xr-no-icon {\n",
       "  display: inline-block;\n",
       "  vertical-align: middle;\n",
       "  width: 1em;\n",
       "  height: 1.5em !important;\n",
       "  stroke-width: 0;\n",
       "  stroke: currentColor;\n",
       "  fill: currentColor;\n",
       "}\n",
       "</style><pre class='xr-text-repr-fallback'>&lt;xarray.Dataset&gt;\n",
       "Dimensions:  (subject: 60, voxel: 1989, time: 284)\n",
       "Coordinates:\n",
       "  * voxel    (voxel) int64 0 1 2 3 4 5 6 ... 1982 1983 1984 1985 1986 1987 1988\n",
       "  * time     (time) int64 0 1 2 3 4 5 6 7 8 ... 276 277 278 279 280 281 282 283\n",
       "  * subject  (subject) object &#x27;118932_RL&#x27; &#x27;113619_LR&#x27; ... &#x27;148335_LR&#x27;\n",
       "    task     object ...\n",
       "Data variables:\n",
       "    X        (subject, voxel, time) float64 ...\n",
       "    Y        (subject, voxel, time) float64 ...\n",
       "    Y_conv   (subject, voxel, time) float64 ...\n",
       "Attributes:\n",
       "    description:  Dataset for subject 118932, task MOTOR, acquisition RL</pre><div class='xr-wrap' style='display:none'><div class='xr-header'><div class='xr-obj-type'>xarray.Dataset</div></div><ul class='xr-sections'><li class='xr-section-item'><input id='section-a1ce5cbd-ef3c-4c3f-bcb1-e9ceea84388c' class='xr-section-summary-in' type='checkbox' disabled ><label for='section-a1ce5cbd-ef3c-4c3f-bcb1-e9ceea84388c' class='xr-section-summary'  title='Expand/collapse section'>Dimensions:</label><div class='xr-section-inline-details'><ul class='xr-dim-list'><li><span class='xr-has-index'>subject</span>: 60</li><li><span class='xr-has-index'>voxel</span>: 1989</li><li><span class='xr-has-index'>time</span>: 284</li></ul></div><div class='xr-section-details'></div></li><li class='xr-section-item'><input id='section-da697879-0f87-45f3-a7f0-9934ef79bd1e' class='xr-section-summary-in' type='checkbox'  checked><label for='section-da697879-0f87-45f3-a7f0-9934ef79bd1e' class='xr-section-summary' >Coordinates: <span>(4)</span></label><div class='xr-section-inline-details'></div><div class='xr-section-details'><ul class='xr-var-list'><li class='xr-var-item'><div class='xr-var-name'><span class='xr-has-index'>voxel</span></div><div class='xr-var-dims'>(voxel)</div><div class='xr-var-dtype'>int64</div><div class='xr-var-preview xr-preview'>0 1 2 3 4 ... 1985 1986 1987 1988</div><input id='attrs-bc40624e-a3ee-4272-95fa-ebfd84bf92b0' class='xr-var-attrs-in' type='checkbox' disabled><label for='attrs-bc40624e-a3ee-4272-95fa-ebfd84bf92b0' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-77cf9015-7842-42c5-8c26-66e8c91e3a2e' class='xr-var-data-in' type='checkbox'><label for='data-77cf9015-7842-42c5-8c26-66e8c91e3a2e' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'></dl></div><div class='xr-var-data'><pre>array([   0,    1,    2, ..., 1986, 1987, 1988])</pre></div></li><li class='xr-var-item'><div class='xr-var-name'><span class='xr-has-index'>time</span></div><div class='xr-var-dims'>(time)</div><div class='xr-var-dtype'>int64</div><div class='xr-var-preview xr-preview'>0 1 2 3 4 5 ... 279 280 281 282 283</div><input id='attrs-1fb79203-0225-447f-a1d0-6a732fa1d0d6' class='xr-var-attrs-in' type='checkbox' disabled><label for='attrs-1fb79203-0225-447f-a1d0-6a732fa1d0d6' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-d7bd23d2-7a13-4e77-bb9d-526aa9cf45c8' class='xr-var-data-in' type='checkbox'><label for='data-d7bd23d2-7a13-4e77-bb9d-526aa9cf45c8' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'></dl></div><div class='xr-var-data'><pre>array([  0,   1,   2, ..., 281, 282, 283])</pre></div></li><li class='xr-var-item'><div class='xr-var-name'><span class='xr-has-index'>subject</span></div><div class='xr-var-dims'>(subject)</div><div class='xr-var-dtype'>object</div><div class='xr-var-preview xr-preview'>&#x27;118932_RL&#x27; ... &#x27;148335_LR&#x27;</div><input id='attrs-a23fff6a-511c-4e29-82e4-8042205b3b47' class='xr-var-attrs-in' type='checkbox' disabled><label for='attrs-a23fff6a-511c-4e29-82e4-8042205b3b47' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-19d69d5b-fa7b-43c5-8d70-4f2ee7728f13' class='xr-var-data-in' type='checkbox'><label for='data-19d69d5b-fa7b-43c5-8d70-4f2ee7728f13' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'></dl></div><div class='xr-var-data'><pre>array([&#x27;118932_RL&#x27;, &#x27;113619_LR&#x27;, &#x27;211417_LR&#x27;, &#x27;239944_LR&#x27;, &#x27;103818_LR&#x27;,\n",
       "       &#x27;149741_RL&#x27;, &#x27;113619_RL&#x27;, &#x27;101915_RL&#x27;, &#x27;118528_RL&#x27;, &#x27;298051_RL&#x27;,\n",
       "       &#x27;397760_LR&#x27;, &#x27;136833_RL&#x27;, &#x27;190031_LR&#x27;, &#x27;178950_RL&#x27;, &#x27;115320_RL&#x27;,\n",
       "       &#x27;211417_RL&#x27;, &#x27;672756_LR&#x27;, &#x27;122620_LR&#x27;, &#x27;214423_LR&#x27;, &#x27;135932_RL&#x27;,\n",
       "       &#x27;189450_LR&#x27;, &#x27;122317_RL&#x27;, &#x27;151627_RL&#x27;, &#x27;118528_LR&#x27;, &#x27;414229_RL&#x27;,\n",
       "       &#x27;159340_RL&#x27;, &#x27;140925_RL&#x27;, &#x27;149741_LR&#x27;, &#x27;133019_LR&#x27;, &#x27;366446_RL&#x27;,\n",
       "       &#x27;106016_LR&#x27;, &#x27;101915_LR&#x27;, &#x27;672756_RL&#x27;, &#x27;298051_LR&#x27;, &#x27;130316_LR&#x27;,\n",
       "       &#x27;751348_LR&#x27;, &#x27;176542_RL&#x27;, &#x27;101107_RL&#x27;, &#x27;124422_LR&#x27;, &#x27;138534_RL&#x27;,\n",
       "       &#x27;118730_RL&#x27;, &#x27;100408_RL&#x27;, &#x27;190031_RL&#x27;, &#x27;201111_LR&#x27;, &#x27;239944_RL&#x27;,\n",
       "       &#x27;127630_RL&#x27;, &#x27;161731_RL&#x27;, &#x27;245333_LR&#x27;, &#x27;122620_RL&#x27;, &#x27;129028_LR&#x27;,\n",
       "       &#x27;163129_LR&#x27;, &#x27;196750_RL&#x27;, &#x27;159340_LR&#x27;, &#x27;857263_LR&#x27;, &#x27;103818_RL&#x27;,\n",
       "       &#x27;113922_RL&#x27;, &#x27;499566_LR&#x27;, &#x27;196750_LR&#x27;, &#x27;103111_RL&#x27;, &#x27;148335_LR&#x27;],\n",
       "      dtype=object)</pre></div></li><li class='xr-var-item'><div class='xr-var-name'><span>task</span></div><div class='xr-var-dims'>()</div><div class='xr-var-dtype'>object</div><div class='xr-var-preview xr-preview'>...</div><input id='attrs-cb83afde-8adf-454e-a842-172dcd2e79db' class='xr-var-attrs-in' type='checkbox' disabled><label for='attrs-cb83afde-8adf-454e-a842-172dcd2e79db' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-c20bdcdb-2f33-4d49-b3b9-b75e3aac4db3' class='xr-var-data-in' type='checkbox'><label for='data-c20bdcdb-2f33-4d49-b3b9-b75e3aac4db3' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'></dl></div><div class='xr-var-data'><pre>[1 values with dtype=object]</pre></div></li></ul></div></li><li class='xr-section-item'><input id='section-10d76b0d-9eb2-40b5-94ab-34ee9bed056f' class='xr-section-summary-in' type='checkbox'  checked><label for='section-10d76b0d-9eb2-40b5-94ab-34ee9bed056f' class='xr-section-summary' >Data variables: <span>(3)</span></label><div class='xr-section-inline-details'></div><div class='xr-section-details'><ul class='xr-var-list'><li class='xr-var-item'><div class='xr-var-name'><span>X</span></div><div class='xr-var-dims'>(subject, voxel, time)</div><div class='xr-var-dtype'>float64</div><div class='xr-var-preview xr-preview'>...</div><input id='attrs-24ab01b7-53f1-4365-a8ef-2284dac9ae83' class='xr-var-attrs-in' type='checkbox' disabled><label for='attrs-24ab01b7-53f1-4365-a8ef-2284dac9ae83' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-988da9bb-0ea9-4ab9-8970-e694ed1e9729' class='xr-var-data-in' type='checkbox'><label for='data-988da9bb-0ea9-4ab9-8970-e694ed1e9729' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'></dl></div><div class='xr-var-data'><pre>[33892560 values with dtype=float64]</pre></div></li><li class='xr-var-item'><div class='xr-var-name'><span>Y</span></div><div class='xr-var-dims'>(subject, voxel, time)</div><div class='xr-var-dtype'>float64</div><div class='xr-var-preview xr-preview'>...</div><input id='attrs-dd10faa2-1d7a-442c-8b4f-41dc760617cf' class='xr-var-attrs-in' type='checkbox' disabled><label for='attrs-dd10faa2-1d7a-442c-8b4f-41dc760617cf' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-8bbc6879-3a45-4459-8c00-32887886b9ba' class='xr-var-data-in' type='checkbox'><label for='data-8bbc6879-3a45-4459-8c00-32887886b9ba' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'></dl></div><div class='xr-var-data'><pre>[33892560 values with dtype=float64]</pre></div></li><li class='xr-var-item'><div class='xr-var-name'><span>Y_conv</span></div><div class='xr-var-dims'>(subject, voxel, time)</div><div class='xr-var-dtype'>float64</div><div class='xr-var-preview xr-preview'>...</div><input id='attrs-7726b0e8-cf07-499b-a51a-8bd623d8f94c' class='xr-var-attrs-in' type='checkbox' disabled><label for='attrs-7726b0e8-cf07-499b-a51a-8bd623d8f94c' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-17ae0ef4-af91-4e65-8d86-b9691baf7180' class='xr-var-data-in' type='checkbox'><label for='data-17ae0ef4-af91-4e65-8d86-b9691baf7180' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'></dl></div><div class='xr-var-data'><pre>[33892560 values with dtype=float64]</pre></div></li></ul></div></li><li class='xr-section-item'><input id='section-1f81f6a7-2833-451a-b8aa-b14196449474' class='xr-section-summary-in' type='checkbox'  ><label for='section-1f81f6a7-2833-451a-b8aa-b14196449474' class='xr-section-summary' >Indexes: <span>(3)</span></label><div class='xr-section-inline-details'></div><div class='xr-section-details'><ul class='xr-var-list'><li class='xr-var-item'><div class='xr-index-name'><div>voxel</div></div><div class='xr-index-preview'>PandasIndex</div><div></div><input id='index-17f637cc-6d2c-4ac9-86b8-e6f16d8074d7' class='xr-index-data-in' type='checkbox'/><label for='index-17f637cc-6d2c-4ac9-86b8-e6f16d8074d7' title='Show/Hide index repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-index-data'><pre>PandasIndex(Int64Index([   0,    1,    2,    3,    4,    5,    6,    7,    8,    9,\n",
       "            ...\n",
       "            1979, 1980, 1981, 1982, 1983, 1984, 1985, 1986, 1987, 1988],\n",
       "           dtype=&#x27;int64&#x27;, name=&#x27;voxel&#x27;, length=1989))</pre></div></li><li class='xr-var-item'><div class='xr-index-name'><div>time</div></div><div class='xr-index-preview'>PandasIndex</div><div></div><input id='index-b40a5961-5271-4f60-9293-b0a67365d532' class='xr-index-data-in' type='checkbox'/><label for='index-b40a5961-5271-4f60-9293-b0a67365d532' title='Show/Hide index repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-index-data'><pre>PandasIndex(Int64Index([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,\n",
       "            ...\n",
       "            274, 275, 276, 277, 278, 279, 280, 281, 282, 283],\n",
       "           dtype=&#x27;int64&#x27;, name=&#x27;time&#x27;, length=284))</pre></div></li><li class='xr-var-item'><div class='xr-index-name'><div>subject</div></div><div class='xr-index-preview'>PandasIndex</div><div></div><input id='index-57b4a4d9-3833-44a6-abcf-2d9a10a0c064' class='xr-index-data-in' type='checkbox'/><label for='index-57b4a4d9-3833-44a6-abcf-2d9a10a0c064' title='Show/Hide index repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-index-data'><pre>PandasIndex(Index([&#x27;118932_RL&#x27;, &#x27;113619_LR&#x27;, &#x27;211417_LR&#x27;, &#x27;239944_LR&#x27;, &#x27;103818_LR&#x27;,\n",
       "       &#x27;149741_RL&#x27;, &#x27;113619_RL&#x27;, &#x27;101915_RL&#x27;, &#x27;118528_RL&#x27;, &#x27;298051_RL&#x27;,\n",
       "       &#x27;397760_LR&#x27;, &#x27;136833_RL&#x27;, &#x27;190031_LR&#x27;, &#x27;178950_RL&#x27;, &#x27;115320_RL&#x27;,\n",
       "       &#x27;211417_RL&#x27;, &#x27;672756_LR&#x27;, &#x27;122620_LR&#x27;, &#x27;214423_LR&#x27;, &#x27;135932_RL&#x27;,\n",
       "       &#x27;189450_LR&#x27;, &#x27;122317_RL&#x27;, &#x27;151627_RL&#x27;, &#x27;118528_LR&#x27;, &#x27;414229_RL&#x27;,\n",
       "       &#x27;159340_RL&#x27;, &#x27;140925_RL&#x27;, &#x27;149741_LR&#x27;, &#x27;133019_LR&#x27;, &#x27;366446_RL&#x27;,\n",
       "       &#x27;106016_LR&#x27;, &#x27;101915_LR&#x27;, &#x27;672756_RL&#x27;, &#x27;298051_LR&#x27;, &#x27;130316_LR&#x27;,\n",
       "       &#x27;751348_LR&#x27;, &#x27;176542_RL&#x27;, &#x27;101107_RL&#x27;, &#x27;124422_LR&#x27;, &#x27;138534_RL&#x27;,\n",
       "       &#x27;118730_RL&#x27;, &#x27;100408_RL&#x27;, &#x27;190031_RL&#x27;, &#x27;201111_LR&#x27;, &#x27;239944_RL&#x27;,\n",
       "       &#x27;127630_RL&#x27;, &#x27;161731_RL&#x27;, &#x27;245333_LR&#x27;, &#x27;122620_RL&#x27;, &#x27;129028_LR&#x27;,\n",
       "       &#x27;163129_LR&#x27;, &#x27;196750_RL&#x27;, &#x27;159340_LR&#x27;, &#x27;857263_LR&#x27;, &#x27;103818_RL&#x27;,\n",
       "       &#x27;113922_RL&#x27;, &#x27;499566_LR&#x27;, &#x27;196750_LR&#x27;, &#x27;103111_RL&#x27;, &#x27;148335_LR&#x27;],\n",
       "      dtype=&#x27;object&#x27;, name=&#x27;subject&#x27;))</pre></div></li></ul></div></li><li class='xr-section-item'><input id='section-8192ca57-462a-41b3-84b4-23aa66674c15' class='xr-section-summary-in' type='checkbox'  checked><label for='section-8192ca57-462a-41b3-84b4-23aa66674c15' class='xr-section-summary' >Attributes: <span>(1)</span></label><div class='xr-section-inline-details'></div><div class='xr-section-details'><dl class='xr-attrs'><dt><span>description :</span></dt><dd>Dataset for subject 118932, task MOTOR, acquisition RL</dd></dl></div></li></ul></div></div>"
      ],
      "text/plain": [
       "<xarray.Dataset>\n",
       "Dimensions:  (subject: 60, voxel: 1989, time: 284)\n",
       "Coordinates:\n",
       "  * voxel    (voxel) int64 0 1 2 3 4 5 6 ... 1982 1983 1984 1985 1986 1987 1988\n",
       "  * time     (time) int64 0 1 2 3 4 5 6 7 8 ... 276 277 278 279 280 281 282 283\n",
       "  * subject  (subject) object '118932_RL' '113619_LR' ... '148335_LR'\n",
       "    task     object ...\n",
       "Data variables:\n",
       "    X        (subject, voxel, time) float64 ...\n",
       "    Y        (subject, voxel, time) float64 ...\n",
       "    Y_conv   (subject, voxel, time) float64 ...\n",
       "Attributes:\n",
       "    description:  Dataset for subject 118932, task MOTOR, acquisition RL"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "131be007-23ce-49ab-a11a-ead8b94b8e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "\n",
    "def create_train_test_split(dataset, test_size=0.2, random_state=None):\n",
    "    subjects = dataset.subject.values\n",
    "    splitter = GroupShuffleSplit(n_splits=1, test_size=test_size, random_state=random_state)\n",
    "    train_idx, test_idx = next(splitter.split(subjects, groups=subjects))\n",
    "    \n",
    "    train_subjects = subjects[train_idx]\n",
    "    test_subjects = subjects[test_idx]\n",
    "    \n",
    "    return dataset.sel(subject=train_subjects), dataset.sel(subject=test_subjects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "864afd20-cb81-4903-895f-ee201b8e103f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = create_train_test_split(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dd0f859c-6fcd-4db1-9bb6-3056dbca4ee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.bi_lstm import BiLSTMModel, BiLSTMTrainer\n",
    "from models.cnn_rnn import CNNRNNModel, CNNRNNTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2d2edced-1137-4b6a-a529-621c79916306",
   "metadata": {},
   "outputs": [],
   "source": [
    "models_and_trainers = [(CNNRNNModel, CNNRNNTrainer), (BiLSTMModel, BiLSTMTrainer)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60f6a0d1-ab23-4205-a046-d5e135537348",
   "metadata": {},
   "source": [
    "# replication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c4ba8647-fed9-4f73-888f-7593c7988d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_data(X_train, Y_train, Y_conv_train, shift_range=(-20, +20), amplitude_range=(0.5, 1.5), noise_std=0.1):\n",
    "    \"\"\"\n",
    "    Augment the data by shifting and scaling the time series data.\n",
    "    \n",
    "    Parameters:\n",
    "    - X_train, Y_train, Y_conv_train: Tensors for the BOLD signal and predicted signals (Y and Y_conv).\n",
    "    - shift_range: Tuple defining the range for temporal shifts (in number of time steps).\n",
    "    - amplitude_range: Tuple defining the range for amplitude scaling (scaling factor).\n",
    "    \n",
    "    Returns:\n",
    "    - Augmented tensors for X, Y, and Y_conv.\n",
    "    \"\"\"\n",
    "    print(\"Data augmentation\")\n",
    "    # Apply time shifts and amplitude scaling to the training set\n",
    "    augmented_X = []\n",
    "    augmented_Y = []\n",
    "    augmented_Y_conv = []\n",
    "    \n",
    "    num_samples = X_train.shape[0]\n",
    "    \n",
    "    for i in range(num_samples):\n",
    "        # 1. Apply random time shift\n",
    "        shift = np.random.randint(shift_range[0], shift_range[1] + 1)  # Random shift between shift_range[0] and shift_range[1]\n",
    "        shifted_X = torch.roll(X_train[i], shifts=shift, dims=0)\n",
    "        shifted_Y = torch.roll(Y_train[i], shifts=shift, dims=0)\n",
    "        shifted_Y_conv = torch.roll(Y_conv_train[i], shifts=shift, dims=0)\n",
    "\n",
    "        # 2. Apply random amplitude scaling\n",
    "        scale_factor = np.random.uniform(amplitude_range[0], amplitude_range[1])\n",
    "        scaled_X = shifted_X * scale_factor\n",
    "        scaled_Y = shifted_Y * scale_factor\n",
    "        scaled_Y_conv = shifted_Y_conv * scale_factor\n",
    "\n",
    "        # 3. Add Gaussian noise\n",
    "        noise_X = torch.normal(mean=0, std=noise_std, size=scaled_X.shape).to(scaled_X.device)\n",
    "        \n",
    "        # Add the noise to the scaled data\n",
    "        noisy_X = scaled_X + noise_X\n",
    "        \n",
    "        # Store augmented samples\n",
    "        augmented_X.append(noisy_X)\n",
    "        augmented_Y.append(scaled_Y)\n",
    "        augmented_Y_conv.append(scaled_Y_conv)\n",
    "\n",
    "    # Convert lists back to tensors\n",
    "    augmented_X_tensor = torch.stack(augmented_X)\n",
    "    augmented_Y_tensor = torch.stack(augmented_Y)\n",
    "    augmented_Y_conv_tensor = torch.stack(augmented_Y_conv)\n",
    "\n",
    "    return augmented_X_tensor, augmented_Y_tensor, augmented_Y_conv_tensor\n",
    "\n",
    "\n",
    "def preprocess_subject_data(subject_data):\n",
    "    \"\"\"\n",
    "    Preprocess the data for a given subject, including normalization, \n",
    "    handling missing values, and applying temporal shifts.\n",
    "\n",
    "    Parameters:\n",
    "    - subject_data: The data for a single subject.\n",
    "\n",
    "    Returns:\n",
    "    - X_tensor, Y_tensor, Y_conv_tensor: Preprocessed tensors for the BOLD signal and predicted signals.\n",
    "    \"\"\"\n",
    "    #print(\"Data preprocess\")\n",
    "    \n",
    "    X_subject = subject_data['X']  # Shape: [voxels, time]\n",
    "    Y_subject = subject_data['Y']  # Shape: [voxels, time]\n",
    "    Y_conv_subject = subject_data['Y_conv']  # Shape: [voxels, time]\n",
    "\n",
    "    # Access time series for all voxels\n",
    "    X_vals = X_subject.values\n",
    "    Y_vals = Y_subject.values\n",
    "    Y_conv_vals = Y_conv_subject.values\n",
    "\n",
    "    # Remove rows with NaNs in any voxel time series\n",
    "    #print(\"Remove rows with NaNs in any voxel time series\")\n",
    "    valid_mask = ~np.isnan(X_vals).any(axis=1)\n",
    "    X_vals = X_vals[valid_mask]\n",
    "    Y_vals = Y_vals[valid_mask]\n",
    "    Y_conv_vals = Y_conv_vals[valid_mask]\n",
    "\n",
    "    # Normalize BOLD signal (X)\n",
    "    #print(\"Normalize BOLD signal (X)\")\n",
    "    X_vals = (X_vals - np.mean(X_vals, axis=1, keepdims=True)) / np.std(X_vals, axis=1, keepdims=True)\n",
    "\n",
    "    # Convert to PyTorch tensors\n",
    "    #print(\"Convert to PyTorch tensors\")\n",
    "    X_tensor = torch.tensor(X_vals, dtype=torch.float32)\n",
    "    Y_tensor = torch.tensor(Y_vals, dtype=torch.float32)\n",
    "    Y_conv_tensor = torch.tensor(Y_conv_vals, dtype=torch.float32)\n",
    "\n",
    "    return X_tensor, Y_tensor, Y_conv_tensor\n",
    "\n",
    "\n",
    "def split_and_shuffle_data(X_tensor, Y_tensor, Y_conv_tensor):\n",
    "    \"\"\"\n",
    "    Shuffle and split the data into training, validation, and test sets.\n",
    "\n",
    "    Parameters:\n",
    "    - X_tensor, Y_tensor, Y_conv_tensor: Tensors for the BOLD signal and predicted signals.\n",
    "\n",
    "    Returns:\n",
    "    - X_train, X_val, X_test: Split training, validation, and test sets for BOLD signal.\n",
    "    - Y_train, Y_val, Y_test: Split training, validation, and test sets for predicted signals.\n",
    "    - Y_conv_train, Y_conv_val, Y_conv_test: Split training, validation, and test sets for predicted signals (Y_conv).\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"split_and_shuffle_data\")\n",
    "    num_samples = X_tensor.shape[0]\n",
    "    indices = np.random.permutation(num_samples)\n",
    "    X_tensor = X_tensor[indices]\n",
    "    Y_tensor = Y_tensor[indices]\n",
    "    Y_conv_tensor = Y_conv_tensor[indices]\n",
    "\n",
    "    # Split data into train, validation, and test sets (60%, 20%, 20%)\n",
    "    train_end = int(0.6 * num_samples)\n",
    "    val_end = train_end + int(0.2 * num_samples)\n",
    "\n",
    "    # Use torch.split for easier splitting\n",
    "    X_train, X_val, X_test = torch.split(X_tensor, [train_end, val_end - train_end, num_samples - val_end])\n",
    "    Y_train, Y_val, Y_test = torch.split(Y_tensor, [train_end, val_end - train_end, num_samples - val_end])\n",
    "    Y_conv_train, Y_conv_val, Y_conv_test = torch.split(Y_conv_tensor, [train_end, val_end - train_end, num_samples - val_end])\n",
    "\n",
    "    return X_train, X_val, X_test, Y_train, Y_val, Y_test, Y_conv_train, Y_conv_val, Y_conv_test\n",
    "\n",
    "\n",
    "def concat_data(all_X_train, all_X_val, all_X_test, all_Y_train, all_Y_val, all_Y_test, all_Y_conv_train, all_Y_conv_val, all_Y_conv_test):\n",
    "    \"\"\"\n",
    "    Concatenate the data from all subjects into one large dataset.\n",
    "\n",
    "    Parameters:\n",
    "    - all_X_train, all_X_val, all_X_test, all_Y_train, all_Y_val, all_Y_test, all_Y_conv_train, all_Y_conv_val, all_Y_conv_test: \n",
    "      Lists of tensors containing data for each subject.\n",
    "\n",
    "    Returns:\n",
    "    - Concatenated tensors for training, validation, and test sets.\n",
    "    \"\"\"\n",
    "        \n",
    "    X_train_def = torch.cat(all_X_train, dim=0).unsqueeze(-1)\n",
    "    \n",
    "    for x_train in all_X_train:\n",
    "        print(\"xtrain stuff\", x_train.shape, X_train_def.shape)\n",
    "\n",
    "    X_test_def = torch.cat(all_X_test, dim=0).unsqueeze(-1)\n",
    "    X_val_def = torch.cat(all_X_val, dim=0).unsqueeze(-1)\n",
    "    Y_train_def = torch.cat(all_Y_train, dim=0).unsqueeze(-1)\n",
    "    Y_test_def = torch.cat(all_Y_test, dim=0).unsqueeze(-1)\n",
    "    Y_val_def = torch.cat(all_Y_val, dim=0).unsqueeze(-1)\n",
    "    Y_conv_train_def = torch.cat(all_Y_conv_train, dim=0).unsqueeze(-1)\n",
    "    Y_conv_test_def = torch.cat(all_Y_conv_test, dim=0).unsqueeze(-1)\n",
    "    Y_conv_val_def = torch.cat(all_Y_conv_val, dim=0).unsqueeze(-1)\n",
    "    \n",
    "    return X_train_def, X_test_def, X_val_def, Y_train_def, Y_test_def, Y_val_def, Y_conv_train_def, Y_conv_test_def, Y_conv_val_def\n",
    "\n",
    "def get_data_tensors(subject_ids):\n",
    "    \"\"\"\n",
    "    Function to load, preprocess, and split data for a given list of subjects.\n",
    "    It normalizes the BOLD signal (X), applies temporal shifts (if any),\n",
    "    and splits the data into training, validation, and test sets.\n",
    "\n",
    "    Parameters:\n",
    "    - subject_ids: List of subject IDs to process.\n",
    "\n",
    "    Returns:\n",
    "    - X_train, Y_train, Y_conv_train: Training data tensors for BOLD signal and predicted signals.\n",
    "    - X_test, Y_test, Y_conv_test: Test data tensors for BOLD signal and predicted signals.\n",
    "    - X_val, Y_val, Y_conv_val: Validation data tensors for BOLD signal and predicted signals.\n",
    "    \"\"\"\n",
    "\n",
    "    all_X_train, all_X_val, all_X_test = [], [], []\n",
    "    all_Y_train, all_Y_val, all_Y_test = [], [], []\n",
    "    all_Y_conv_train, all_Y_conv_val, all_Y_conv_test = [], [], []\n",
    "\n",
    "    # Process data for each subject\n",
    "    for sub_id in subject_ids:\n",
    "        print(f\"Processing subject: {sub_id}\")\n",
    "        subject_data = dataset.sel(subject=sub_id)\n",
    "\n",
    "        print(\"subject_data shape\", subject_data.sizes)\n",
    "        \n",
    "        # Extract time series for BOLD and predicted signals\n",
    "        X_tensor, Y_tensor, Y_conv_tensor = preprocess_subject_data(subject_data)\n",
    "        print(\"X_tensor shape\", X_tensor.shape)\n",
    "        \n",
    "        # Shuffle and split data into train, val, and test sets\n",
    "        X_train, X_val, X_test, Y_train, Y_val, Y_test, Y_conv_train, Y_conv_val, Y_conv_test = split_and_shuffle_data(\n",
    "            X_tensor, Y_tensor, Y_conv_tensor)\n",
    "\n",
    "        # Apply data augmentation on the training set\n",
    "        X_train_aug, Y_train_aug, Y_conv_train_aug = X_train, Y_train, Y_conv_train\n",
    "\n",
    "        # Append the results for each subject\n",
    "        # each for cycle, shift and amplitude for augmentation can change, in order to have variability\n",
    "        all_X_train.append(X_train_aug)\n",
    "        all_X_val.append(X_val)\n",
    "        all_X_test.append(X_test)\n",
    "        all_Y_train.append(Y_train_aug)\n",
    "        all_Y_val.append(Y_val)\n",
    "        all_Y_test.append(Y_test)\n",
    "        all_Y_conv_train.append(Y_conv_train_aug)\n",
    "        all_Y_conv_val.append(Y_conv_val)\n",
    "        all_Y_conv_test.append(Y_conv_test)\n",
    "\n",
    "    # Concatenate data for all subjects\n",
    "    print(\"pre concat\"\n",
    "    X_train_def, X_test_def, X_val_def, Y_train_def, Y_test_def, Y_val_def, Y_conv_train_def, Y_conv_test_def, Y_conv_val_def = concat_data(\n",
    "        all_X_train, all_X_val, all_X_test, all_Y_train, all_Y_val, all_Y_test, all_Y_conv_train, all_Y_conv_val, all_Y_conv_test\n",
    "    )\n",
    "\n",
    "    print(f\"Training data shape: {X_train_def.shape}, Test data shape: {X_test_def.shape}\")\n",
    "    print(f\"Labels (Y) shapes: Train {Y_train_def.shape}, Test {Y_test_def.shape}\")\n",
    "\n",
    "    return X_train_def, Y_train_def, Y_conv_train_def, X_test_def, Y_test_def, Y_conv_test_def, X_val_def, Y_val_def, Y_conv_val_def\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9acca152-69fe-442f-b910-b07dd112499e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing subject: 118932_RL\n",
      "subject_data shape Frozen({'voxel': 1989, 'time': 284})\n",
      "X_tensor shape torch.Size([1959, 284])\n",
      "split_and_shuffle_data\n",
      "Processing subject: 113619_LR\n",
      "subject_data shape Frozen({'voxel': 1989, 'time': 284})\n",
      "X_tensor shape torch.Size([1930, 284])\n",
      "split_and_shuffle_data\n",
      "Processing subject: 211417_LR\n",
      "subject_data shape Frozen({'voxel': 1989, 'time': 284})\n",
      "X_tensor shape torch.Size([1939, 284])\n",
      "split_and_shuffle_data\n",
      "xtrain stuff torch.Size([1175, 284]) torch.Size([3496, 284, 1])\n",
      "xtrain stuff torch.Size([1158, 284]) torch.Size([3496, 284, 1])\n",
      "xtrain stuff torch.Size([1163, 284]) torch.Size([3496, 284, 1])\n",
      "Training data shape: torch.Size([3496, 284, 1]), Test data shape: torch.Size([1168, 284, 1])\n",
      "Labels (Y) shapes: Train torch.Size([3496, 284, 1]), Test torch.Size([1168, 284, 1])\n"
     ]
    }
   ],
   "source": [
    "subjects_ids = dataset.subject.values[:3]\n",
    "\n",
    "X_train_def, Y_train_def, Y_conv_train_def, X_test_def, Y_test_def, Y_conv_test_def, X_val_def, Y_val_def, Y_conv_val_def = get_data_tensors(subjects_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "756e14be-fcdf-43c4-be20-8ead51fe1e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Training and evaluation function\n",
    "def train_model(model, criterion, train_loader, validation_loader, optimizer, num_epochs, device):\n",
    "    train_losses = []  # To store average training loss for each epoch\n",
    "    validation_losses = []\n",
    "    for epoch in range(num_epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        total_train_loss = 0\n",
    "        for batch_x, batch_y in train_loader:\n",
    "            batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "            #print(batch_x.shape) #(batch_size,timepoints,1)\n",
    "            # Forward pass\n",
    "            predictions = model(batch_x)\n",
    "            loss = criterion(predictions, batch_y)\n",
    "            total_train_loss += loss.item()\n",
    "\n",
    "            # Backward pass\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        train_losses.append(avg_train_loss)  # Store train loss\n",
    "        avg_train_loss = total_train_loss / len(train_loader)\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {avg_train_loss:.4f}\")\n",
    "\n",
    "        # Evaluation phase\n",
    "        model.eval()\n",
    "        total_validation_loss = 0\n",
    "        total_validation_mape = 0\n",
    "        with torch.no_grad():\n",
    "            for batch_x, batch_y in validation_loader:\n",
    "                batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "                predictions = model(batch_x)\n",
    "                loss = criterion(predictions, batch_y)\n",
    "                total_validation_loss += loss.item()\n",
    "        avg_validation_loss = total_validation_loss / len(validation_loader)\n",
    "        validation_losses.append(avg_validation_loss)  # Store test loss\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, validation Loss: {avg_validation_loss:.4f}\")\n",
    "    # Plot the losses after training\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(range(1, num_epochs + 1), train_losses, label='Train Loss', marker='o')\n",
    "    plt.plot(range(1, num_epochs + 1), validation_losses, label='validation Loss', marker='o')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.yscale('log') \n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Train vs Test Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "# Training and evaluation function with early stopping\n",
    "def train_model_with_early_stopping(\n",
    "    model, criterion, train_loader, validation_loader, optimizer, num_epochs, device, patience=5\n",
    "):\n",
    "    train_losses = []  # To store average training loss for each epoch\n",
    "    validation_losses = []   # To store average test loss for each epoch\n",
    "    best_test_loss = float('inf')  # Initialize best test loss as infinity\n",
    "    patience_counter = 0  # Counter to track epochs without improvement\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        total_train_loss = 0\n",
    "        for batch_x, batch_y in train_loader:\n",
    "            batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            predictions = model(batch_x)\n",
    "\n",
    "            # Check shapes\n",
    "            if predictions.shape != batch_y.shape:\n",
    "                raise ValueError(\n",
    "                    f\"Shape mismatch: Predictions {predictions.shape} vs Targets {batch_y.shape}. \"\n",
    "                    \"Ensure model output matches target dimensions.\"\n",
    "                )\n",
    "\n",
    "            \n",
    "            loss = criterion(predictions, batch_y)\n",
    "            total_train_loss += loss.item()\n",
    "\n",
    "            # Backward pass\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        avg_train_loss = total_train_loss / len(train_loader)\n",
    "        train_losses.append(avg_train_loss)  # Store train loss\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {avg_train_loss:.4f}\")\n",
    "\n",
    "        # Evaluation phase\n",
    "        model.eval()\n",
    "        total_validation_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for batch_x, batch_y in validation_loader:\n",
    "                batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "                predictions = model(batch_x)\n",
    "                loss = criterion(predictions, batch_y)\n",
    "                total_validation_loss += loss.item()\n",
    "\n",
    "        avg_validation_loss = total_validation_loss / len(validation_loader)\n",
    "        validation_losses.append(avg_validation_loss)  # Store test loss\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f\"Epoch {epoch+1}/{num_epochs}, Validation Loss: {avg_validation_loss:.4f}\")\n",
    "\n",
    "        # Early stopping logic\n",
    "        if avg_validation_loss < best_test_loss:\n",
    "            best_test_loss = avg_validation_loss\n",
    "            patience_counter = 0  # Reset patience counter\n",
    "            best_model_state = model.state_dict()  # Save best model state\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            print(f\"No improvement for {patience_counter} epoch(s).\")\n",
    "        \n",
    "        if patience_counter >= patience:\n",
    "            print(f\"Early stopping triggered at epoch {epoch+1}.\")\n",
    "            break\n",
    "\n",
    "        #NEW!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "        # Step the scheduler with the validation loss\n",
    "        scheduler.step(avg_validation_loss)  # Adjust the learning rate based on validation loss\n",
    "\n",
    "\n",
    "    # Restore the best model weights\n",
    "    #model.load_state_dict(best_model_state)\n",
    "\n",
    "    # Plot the losses\n",
    "    plt.figure(figsize=(20, 10))\n",
    "    plt.plot(range(1, len(train_losses) + 1), train_losses, label='Train Loss', marker='o')\n",
    "    plt.plot(range(1, len(validation_losses) + 1), validation_losses, label='validation Loss', marker='o')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.yscale('log') \n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Train vs Validation Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    return best_model_state\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "da8524e2-6b5b-408a-85ef-7e2afe0403b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Hyperparameters\n",
    "input_size = 1  # input is a single feature per voxel\n",
    "hidden_size = 64\n",
    "output_size = 1  # we predict one value per timeseries per voxel\n",
    "num_epochs = 100\n",
    "batch_size = 32\n",
    "learning_rate = 0.0001\n",
    "patience=10\n",
    "\n",
    "X_train_def.to(device);\n",
    "Y_train_def.to(device);\n",
    "X_val_def.to(device);\n",
    "Y_val_def.to(device);\n",
    "\n",
    "# X_train_def = apply_lowpass_filter(X_train_def)\n",
    "# X_val_def = apply_lowpass_filter(X_val_def)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "29dcfe7c-b896-4a37-bf7c-34c94ac74ab7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3496, 284, 1]) torch.Size([3496, 284, 1])\n"
     ]
    }
   ],
   "source": [
    "print(X_train_def.shape, Y_train_def.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "ccdaa4e8-27d2-462c-902d-9682dae34780",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[68], line 18\u001b[0m\n\u001b[1;32m     14\u001b[0m scheduler \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mlr_scheduler\u001b[38;5;241m.\u001b[39mReduceLROnPlateau(optimizer, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmin\u001b[39m\u001b[38;5;124m'\u001b[39m, patience\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m, factor\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m#train_model(model, criterion, train_loader, test_loader, optimizer, num_epochs, device)\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m best_model_state \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model_with_early_stopping\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43mpatience\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpatience\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[65], line 82\u001b[0m, in \u001b[0;36mtrain_model_with_early_stopping\u001b[0;34m(model, criterion, train_loader, validation_loader, optimizer, num_epochs, device, patience)\u001b[0m\n\u001b[1;32m     80\u001b[0m     \u001b[38;5;66;03m# Backward pass\u001b[39;00m\n\u001b[1;32m     81\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 82\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     83\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     85\u001b[0m avg_train_loss \u001b[38;5;241m=\u001b[39m total_train_loss \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(train_loader)\n",
      "File \u001b[0;32m~/.miniconda3/envs/ML4S/lib/python3.8/site-packages/torch/_tensor.py:488\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    478\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    479\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    480\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    481\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    486\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    487\u001b[0m     )\n\u001b[0;32m--> 488\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    490\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.miniconda3/envs/ML4S/lib/python3.8/site-packages/torch/autograd/__init__.py:197\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    192\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    194\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    196\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 197\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    198\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_dataset = TensorDataset(X_train_def, Y_train_def)\n",
    "validation_dataset = TensorDataset(X_val_def, Y_val_def)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "validation_loader = DataLoader(validation_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Model, Loss, Optimizer\n",
    "model = RNNModel(input_size, hidden_size, output_size).to(device)\n",
    "#model = nn.DataParallel(model)\n",
    "#model.to(device)\n",
    "criterion = nn.L1Loss()  # MAE Loss\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-5)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=3, factor=0.1)\n",
    "\n",
    "# Train the model\n",
    "#train_model(model, criterion, train_loader, test_loader, optimizer, num_epochs, device)\n",
    "best_model_state = train_model_with_early_stopping(model, criterion, train_loader, validation_loader, optimizer, num_epochs, device,patience=patience)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "113f7817-bf27-4da0-bb89-5f077143b2b0",
   "metadata": {},
   "source": [
    "# cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "6d0b2afc-966b-4876-926b-f074c4730866",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "import optuna\n",
    "\n",
    "def select_model(models_and_trainers, X_train, y_train, n_trials=100, n_folds=5):\n",
    "    best_score = float('inf')\n",
    "    best_results = None\n",
    "    \n",
    "    kf = KFold(n_splits=n_folds, shuffle=True)\n",
    "    \n",
    "    for ModelClass, TrainerClass in models_and_trainers:\n",
    "        def objective(trial):\n",
    "            # Get hyperparameters for this trial\n",
    "            params = TrainerClass.get_optuna_params(trial)\n",
    "            \n",
    "            # Extract model params from full params\n",
    "            model_params = {\n",
    "                'input_size': params['input_size'],\n",
    "                'hidden_size': params['hidden_size'], \n",
    "                'output_size': params['output_size'],\n",
    "                'dropout_prob': params['dropout_prob']\n",
    "            }\n",
    "            \n",
    "            model = ModelClass(**model_params)\n",
    "            trainer = TrainerClass(model=model, config=params)\n",
    "            \n",
    "            # Cross-validation score\n",
    "            fold_scores = []\n",
    "            for train_idx, val_idx in kf.split(X_train):\n",
    "                X_fold_train, X_fold_val = X_train[train_idx], X_train[val_idx]\n",
    "                y_fold_train, y_fold_val = y_train[train_idx], y_train[val_idx]\n",
    "                \n",
    "                trainer.train(X_fold_train, y_fold_train)\n",
    "                fold_scores.append(trainer.evaluate(X_fold_val, y_fold_val))\n",
    "            \n",
    "            return np.mean(fold_scores)\n",
    "        \n",
    "        study = optuna.create_study(direction='minimize')\n",
    "        study.optimize(objective, n_trials=n_trials)\n",
    "        \n",
    "        if study.best_value < best_score:\n",
    "            best_score = study.best_value\n",
    "            best_results = (ModelClass, TrainerClass, study.best_params)\n",
    "    \n",
    "    if best_results is None:\n",
    "        return None, None, None\n",
    "        \n",
    "    ModelClass, TrainerClass, best_params = best_results\n",
    "    \n",
    "    model_params = {\n",
    "        'input_size': best_params['input_size'],\n",
    "        'hidden_size': best_params['hidden_size'],\n",
    "        'output_size': best_params['output_size'],\n",
    "        'dropout_prob': best_params['dropout_prob']\n",
    "    }\n",
    "    \n",
    "    model = ModelClass(**model_params)\n",
    "    trainer = TrainerClass(model=model, config=best_params)\n",
    "    \n",
    "    return model, trainer, best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "c7644412-c99f-40de-929e-537ef19f8407",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original shape: (60, 1989, 284)\n",
      "Shape after dropping NaNs: (60, 1869, 284)\n",
      "(48, 1869, 284) (12, 1869, 284)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import xarray as xr\n",
    "\n",
    "# from augment import select_augmentation\n",
    "\n",
    "from models.bi_lstm import BiLSTMModel, BiLSTMTrainer\n",
    "from models.cnn_rnn import CNNRNNModel, CNNRNNTrainer\n",
    "\n",
    "DATASET = 'data/derivatives/dataset_MOTOR_60_subjects_both.nc' # hrf convoluted + block, if we want to denoise data\n",
    "\n",
    "def load_data():\n",
    "    return xr.open_dataset(DATASET)\n",
    "\n",
    "def preprocess_dataset(dataset):\n",
    "    valid_mask = ~dataset.X.isnull().any(dim='time')\n",
    "    print(f\"Original shape: {dataset.X.shape}\")\n",
    "    \n",
    "    dataset = dataset.isel(voxel=valid_mask.all(dim='subject'))\n",
    "    \n",
    "    print(f\"Shape after dropping NaNs: {dataset.X.shape}\")\n",
    "    return dataset\n",
    "\n",
    "dataset = preprocess_dataset(load_data())\n",
    "\n",
    "train, test = create_train_test_split(dataset)\n",
    "X_train, X_test, y_train, y_test = train.X, test.X, train.Y, test.Y\n",
    "\n",
    "print(X_train.shape, X_test.shape)\n",
    "\n",
    "#models_and_trainers = [(BiLSTMModel, BiLSTMTrainer), (CNNRNNModel, CNNRNNTrainer)]\n",
    "#best_model, best_trainer_cls, model_params = select_model(\n",
    "#    models_and_trainers, X_train, y_train, n_trials=100, n_folds=5,\n",
    "#)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "a380707b-1dd5-4657-9b3c-c5bf05b58028",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['118932_RL', '113619_LR', '211417_LR', '239944_LR', '103818_LR',\n",
       "       '149741_RL', '113619_RL', '101915_RL', '118528_RL', '298051_RL',\n",
       "       '397760_LR', '136833_RL', '190031_LR', '178950_RL', '115320_RL',\n",
       "       '211417_RL', '672756_LR', '122620_LR', '214423_LR', '135932_RL',\n",
       "       '189450_LR', '122317_RL', '151627_RL', '118528_LR', '414229_RL',\n",
       "       '159340_RL', '140925_RL', '149741_LR', '133019_LR', '366446_RL',\n",
       "       '106016_LR', '101915_LR', '672756_RL', '298051_LR', '130316_LR',\n",
       "       '751348_LR', '176542_RL', '101107_RL', '124422_LR', '138534_RL',\n",
       "       '118730_RL', '100408_RL', '190031_RL', '201111_LR', '239944_RL',\n",
       "       '127630_RL', '161731_RL', '245333_LR', '122620_RL', '129028_LR',\n",
       "       '163129_LR', '196750_RL', '159340_LR', '857263_LR', '103818_RL',\n",
       "       '113922_RL', '499566_LR', '196750_LR', '103111_RL', '148335_LR'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.subject.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b74768-732e-4588-bd7e-f6f238609252",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML4Sc",
   "language": "python",
   "name": "ml4sc"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
