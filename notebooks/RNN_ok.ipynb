{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# new RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ***************************************************\n",
    "# RNN Model \n",
    "# ***************************************************\n",
    "\n",
    "class RNNModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_voxels):\n",
    "        super(RNNModel, self).__init__()\n",
    "        self.rnn = nn.RNN(input_size, hidden_size, batch_first=True)\n",
    "        \n",
    "        # For each time step, predict a vector of beta values for each voxel\n",
    "        self.fc = nn.Linear(hidden_size, num_voxels)  # Output: one prediction per voxel at each time step\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (batch_size, time_steps, num_voxels)\n",
    "        batch_size, time_steps, num_voxels = x.size()  # Correctly unpack the shape\n",
    "        \n",
    "        # Pass through RNN (output: batch_size, time_steps, hidden_size)\n",
    "        rnn_out, _ = self.rnn(x)\n",
    "        \n",
    "        # Apply fully connected layer to predict beta values for each voxel at each time step\n",
    "        # output: (batch_size, time_steps, num_voxels)\n",
    "        output = self.fc(rnn_out)\n",
    "        \n",
    "        return output  # Shape: (batch_size, time_steps, num_voxels)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ***************************************************\n",
    "# Training Function\n",
    "# ***************************************************\n",
    "\n",
    "def train(model, criterion, dataset_train, dataset_test, optimizer, num_epochs, device):\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()  # Set model to training mode\n",
    "        total_loss = 0\n",
    "        \n",
    "        # Iterate through the training data\n",
    "        for batch_x, batch_y in dataset_train:\n",
    "            batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "            \n",
    "            # Reshaping batch_x to (batch_size, time_steps, 1)\n",
    "            batch_x = batch_x.unsqueeze(-1)  # Adding extra dimension for input size\n",
    "            # Ensure the target labels have the correct shape (expand to match (batch_size, time_steps, num_voxels))\n",
    "            batch_y = batch_y.expand(-1, -1, num_voxels)  # Expand to match the output shape (32, 100, 200)\n",
    "            \n",
    "            # Forward pass through the model\n",
    "            predictions = model(batch_x)  # Shape: (batch_size, time_steps, num_voxels)\n",
    "\n",
    "            # Compute loss (Mean Squared Error for regression task)\n",
    "            loss = criterion(predictions, batch_y)  # batch_y: (batch_size, time_steps, num_voxels)\n",
    "            total_loss += loss.item()  # Accumulate the loss over the batch\n",
    "\n",
    "            # Backpropagation\n",
    "            optimizer.zero_grad()  # Zero the gradients before the backward pass\n",
    "            loss.backward()  # Backpropagate the loss\n",
    "            optimizer.step()  # Update the model parameters\n",
    "\n",
    "        # Average loss over the entire training dataset\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {total_loss / len(dataset_train)}\")\n",
    "\n",
    "        # Test the model\n",
    "        model.eval()  # Set model to evaluation mode (disables dropout, batchnorm, etc.)\n",
    "        total_test_loss = 0\n",
    "\n",
    "        # Iterate through the testing data\n",
    "        with torch.no_grad():  # No gradients are needed for evaluation\n",
    "            for batch_x, batch_y in dataset_test:\n",
    "                batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "                batch_x = batch_x.unsqueeze(-1)  # Reshape for evaluation\n",
    "\n",
    "                predictions = model(batch_x)\n",
    "\n",
    "                # Compute the test loss\n",
    "                loss = criterion(predictions, batch_y)\n",
    "                total_test_loss += loss.item()\n",
    "\n",
    "        # Average test loss over the entire test dataset\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Test Loss: {total_test_loss / len(dataset_test)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ***************************************************\n",
    "# Accuracy Calculation\n",
    "# ***************************************************\n",
    "\n",
    "def accuracy(predicted_logits, reference):\n",
    "    \"\"\"\n",
    "    Compute the ratio of correctly predicted labels.\n",
    "    \"\"\"\n",
    "    # Print the shape of predicted logits for debugging\n",
    "    # print(f\"Predicted logits shape: {predicted_logits.shape}\")\n",
    "    # print(f\"Reference shape: {reference.shape}\")\n",
    "\n",
    "    # predicted_logits is expected to be of shape [batch_size * time_steps, num_classes]\n",
    "    # reference is expected to be of shape [batch_size * time_steps]\n",
    "    \n",
    "    # Get predicted class for each time step (the class with the highest logit)\n",
    "    predicted_classes = predicted_logits.argmax(dim=1)  # Shape: [batch_size * time_steps]\n",
    "    \n",
    "    # Count how many predictions match the reference labels\n",
    "    correct_predictions = (predicted_classes == reference).sum()  # Number of correct predictions\n",
    "    accuracy = correct_predictions.float() / reference.numel()  # Compute accuracy\n",
    "    \n",
    "    return accuracy\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ***************************************************\n",
    "# Dataset Preparation\n",
    "# ***************************************************\n",
    "\n",
    "# Generate synthetic data (for example, num_voxels samples)\n",
    "def generate_synthetic_data(num_voxels, time_steps):\n",
    "    # X and Y: Shape (num_voxels, time_steps), i.e., each voxel is a sample\n",
    "    X = torch.randn(num_voxels, time_steps)  # Time series data for each voxel (num_voxels samples)\n",
    "    Y = torch.randn(num_voxels, time_steps)  # Beta values for each voxel at each time step\n",
    "    \n",
    "    # Reshaping Y to match (batch_size, time_steps, num_voxels)\n",
    "    Y = Y.unsqueeze(2)  # Add the voxel dimension\n",
    "    \n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([200, 100])\n",
      "Epoch 1/100, Loss: 1.0027454836027963\n",
      "Epoch 1/100, Test Loss: 0.9857481036867414\n",
      "Epoch 2/100, Loss: 0.9866312657083783\n",
      "Epoch 2/100, Test Loss: 0.9833007454872131\n",
      "Epoch 3/100, Loss: 0.9852571402277265\n",
      "Epoch 3/100, Test Loss: 0.982191128390176\n",
      "Epoch 4/100, Loss: 0.9951179964201791\n",
      "Epoch 4/100, Test Loss: 0.9818691526140485\n",
      "Epoch 5/100, Loss: 0.9915696552821568\n",
      "Epoch 5/100, Test Loss: 0.9817608169146946\n",
      "Epoch 6/100, Loss: 0.9815789546285357\n",
      "Epoch 6/100, Test Loss: 0.9817618812833514\n",
      "Epoch 7/100, Loss: 0.9945144397871835\n",
      "Epoch 7/100, Test Loss: 0.9818718177931649\n",
      "Epoch 8/100, Loss: 0.9813340902328491\n",
      "Epoch 8/100, Test Loss: 0.9817350932529995\n",
      "Epoch 9/100, Loss: 0.9975192206246513\n",
      "Epoch 9/100, Test Loss: 0.9816465888704572\n",
      "Epoch 10/100, Loss: 0.9892709170069013\n",
      "Epoch 10/100, Test Loss: 0.9816653728485107\n",
      "Epoch 11/100, Loss: 0.9884876949446542\n",
      "Epoch 11/100, Test Loss: 0.9816250460488456\n",
      "Epoch 12/100, Loss: 0.9846988575799125\n",
      "Epoch 12/100, Test Loss: 0.9816958819116864\n",
      "Epoch 13/100, Loss: 0.9849622079304287\n",
      "Epoch 13/100, Test Loss: 0.9816798993519374\n",
      "Epoch 14/100, Loss: 0.9887048091207232\n",
      "Epoch 14/100, Test Loss: 0.9815693327358791\n",
      "Epoch 15/100, Loss: 0.9960859673363822\n",
      "Epoch 15/100, Test Loss: 0.9815441455159869\n",
      "Epoch 16/100, Loss: 0.9954236320086888\n",
      "Epoch 16/100, Test Loss: 0.9815896494047982\n",
      "Epoch 17/100, Loss: 0.9989397355488369\n",
      "Epoch 17/100, Test Loss: 0.9816784347806659\n",
      "Epoch 18/100, Loss: 0.982917777129582\n",
      "Epoch 18/100, Test Loss: 0.9816405858312335\n",
      "Epoch 19/100, Loss: 0.9823711088725499\n",
      "Epoch 19/100, Test Loss: 0.9815668974603925\n",
      "Epoch 20/100, Loss: 0.9874199373381478\n",
      "Epoch 20/100, Test Loss: 0.9815310835838318\n",
      "Epoch 21/100, Loss: 0.9916704552514213\n",
      "Epoch 21/100, Test Loss: 0.9815388577325004\n",
      "Epoch 22/100, Loss: 0.9889340315546308\n",
      "Epoch 22/100, Test Loss: 0.9816571133477348\n",
      "Epoch 23/100, Loss: 0.9808945144925799\n",
      "Epoch 23/100, Test Loss: 0.9816633122307914\n",
      "Epoch 24/100, Loss: 0.9878434113093785\n",
      "Epoch 24/100, Test Loss: 0.9817431058202472\n",
      "Epoch 25/100, Loss: 0.9807396616254535\n",
      "Epoch 25/100, Test Loss: 0.981893071106502\n",
      "Epoch 26/100, Loss: 0.9873178856713432\n",
      "Epoch 26/100, Test Loss: 0.9817594034331185\n",
      "Epoch 27/100, Loss: 0.996199471609933\n",
      "Epoch 27/100, Test Loss: 0.9815977045467922\n",
      "Epoch 28/100, Loss: 0.9994748490197318\n",
      "Epoch 28/100, Test Loss: 0.981516991342817\n",
      "Epoch 29/100, Loss: 0.9937078016144889\n",
      "Epoch 29/100, Test Loss: 0.9815298063414437\n",
      "Epoch 30/100, Loss: 0.9812204412051609\n",
      "Epoch 30/100, Test Loss: 0.9815285461289542\n",
      "Epoch 31/100, Loss: 0.9963420033454895\n",
      "Epoch 31/100, Test Loss: 0.9816076244626727\n",
      "Epoch 32/100, Loss: 0.9873736841338021\n",
      "Epoch 32/100, Test Loss: 0.9817650488444737\n",
      "Epoch 33/100, Loss: 0.9988449811935425\n",
      "Epoch 33/100, Test Loss: 0.9817737511226109\n",
      "Epoch 34/100, Loss: 0.9894382357597351\n",
      "Epoch 34/100, Test Loss: 0.9817427056176322\n",
      "Epoch 35/100, Loss: 0.9757406541279384\n",
      "Epoch 35/100, Test Loss: 0.9818442038127354\n",
      "Epoch 36/100, Loss: 0.9897234269550869\n",
      "Epoch 36/100, Test Loss: 0.981745856148856\n",
      "Epoch 37/100, Loss: 0.9852494597434998\n",
      "Epoch 37/100, Test Loss: 0.9820552638598851\n",
      "Epoch 38/100, Loss: 0.9871270997183663\n",
      "Epoch 38/100, Test Loss: 0.9819003939628601\n",
      "Epoch 39/100, Loss: 0.9878060477120536\n",
      "Epoch 39/100, Test Loss: 0.9817363534654889\n",
      "Epoch 40/100, Loss: 0.9884663735117231\n",
      "Epoch 40/100, Test Loss: 0.9817067895616803\n",
      "Epoch 41/100, Loss: 0.9802851251193455\n",
      "Epoch 41/100, Test Loss: 0.9819343345505851\n",
      "Epoch 42/100, Loss: 0.9861735871859959\n",
      "Epoch 42/100, Test Loss: 0.9821250012942723\n",
      "Epoch 43/100, Loss: 0.9957420911107745\n",
      "Epoch 43/100, Test Loss: 0.9818331343787057\n",
      "Epoch 44/100, Loss: 0.9898198332105365\n",
      "Epoch 44/100, Test Loss: 0.9816074286188398\n",
      "Epoch 45/100, Loss: 0.9779861569404602\n",
      "Epoch 45/100, Test Loss: 0.9818589006151471\n",
      "Epoch 46/100, Loss: 0.9886813248906817\n",
      "Epoch 46/100, Test Loss: 0.9818570102964129\n",
      "Epoch 47/100, Loss: 0.9949920858655658\n",
      "Epoch 47/100, Test Loss: 0.9815678255898612\n",
      "Epoch 48/100, Loss: 0.991569561617715\n",
      "Epoch 48/100, Test Loss: 0.981684284550803\n",
      "Epoch 49/100, Loss: 0.9888008747782026\n",
      "Epoch 49/100, Test Loss: 0.9818154403141567\n",
      "Epoch 50/100, Loss: 0.9885348337037223\n",
      "Epoch 50/100, Test Loss: 0.9819790380341666\n",
      "Epoch 51/100, Loss: 0.9875057339668274\n",
      "Epoch 51/100, Test Loss: 0.9824944053377423\n",
      "Epoch 52/100, Loss: 0.9946643710136414\n",
      "Epoch 52/100, Test Loss: 0.9819060819489616\n",
      "Epoch 53/100, Loss: 0.9944412878581456\n",
      "Epoch 53/100, Test Loss: 0.9817860211644854\n",
      "Epoch 54/100, Loss: 0.9850031988961356\n",
      "Epoch 54/100, Test Loss: 0.9821127227374485\n",
      "Epoch 55/100, Loss: 0.9975362590381077\n",
      "Epoch 55/100, Test Loss: 0.9820800253323146\n",
      "Epoch 56/100, Loss: 0.9889208674430847\n",
      "Epoch 56/100, Test Loss: 0.9820559450558254\n",
      "Epoch 57/100, Loss: 0.9803248814174107\n",
      "Epoch 57/100, Test Loss: 0.9824809772627694\n",
      "Epoch 58/100, Loss: 0.9936438628605434\n",
      "Epoch 58/100, Test Loss: 0.9818661894117083\n",
      "Epoch 59/100, Loss: 0.9800179515566144\n",
      "Epoch 59/100, Test Loss: 0.9817110300064087\n",
      "Epoch 60/100, Loss: 0.9904240369796753\n",
      "Epoch 60/100, Test Loss: 0.9818895970072065\n",
      "Epoch 61/100, Loss: 0.9894884484154838\n",
      "Epoch 61/100, Test Loss: 0.9830729109900338\n",
      "Epoch 62/100, Loss: 0.989046607698713\n",
      "Epoch 62/100, Test Loss: 0.9818359017372131\n",
      "Epoch 63/100, Loss: 0.9820108498845782\n",
      "Epoch 63/100, Test Loss: 0.9817266889980861\n",
      "Epoch 64/100, Loss: 0.9784774439675468\n",
      "Epoch 64/100, Test Loss: 0.9820418102400643\n",
      "Epoch 65/100, Loss: 0.9892421024186271\n",
      "Epoch 65/100, Test Loss: 0.9820416143962315\n",
      "Epoch 66/100, Loss: 0.9886107870510646\n",
      "Epoch 66/100, Test Loss: 0.9817409770829337\n",
      "Epoch 67/100, Loss: 0.9904193111828395\n",
      "Epoch 67/100, Test Loss: 0.9818727118628365\n",
      "Epoch 68/100, Loss: 0.9928278497287205\n",
      "Epoch 68/100, Test Loss: 0.9821758610861642\n",
      "Epoch 69/100, Loss: 0.9908462337085179\n",
      "Epoch 69/100, Test Loss: 0.9824392284665789\n",
      "Epoch 70/100, Loss: 0.9918652943202427\n",
      "Epoch 70/100, Test Loss: 0.9823770437921796\n",
      "Epoch 71/100, Loss: 0.9917285186903817\n",
      "Epoch 71/100, Test Loss: 0.9816300017493111\n",
      "Epoch 72/100, Loss: 0.9821802973747253\n",
      "Epoch 72/100, Test Loss: 0.981627379144941\n",
      "Epoch 73/100, Loss: 0.9936270713806152\n",
      "Epoch 73/100, Test Loss: 0.9813747576304844\n",
      "Epoch 74/100, Loss: 0.987900972366333\n",
      "Epoch 74/100, Test Loss: 0.9814036573682513\n",
      "Epoch 75/100, Loss: 0.9891935076032367\n",
      "Epoch 75/100, Test Loss: 0.9821254866463798\n",
      "Epoch 76/100, Loss: 1.0025385958807809\n",
      "Epoch 76/100, Test Loss: 0.9820472938673837\n",
      "Epoch 77/100, Loss: 0.9835947070802961\n",
      "Epoch 77/100, Test Loss: 0.9819574781826564\n",
      "Epoch 78/100, Loss: 0.9832873855318341\n",
      "Epoch 78/100, Test Loss: 0.9822837029184613\n",
      "Epoch 79/100, Loss: 0.9891264864376613\n",
      "Epoch 79/100, Test Loss: 0.9828999638557434\n",
      "Epoch 80/100, Loss: 1.0052690505981445\n",
      "Epoch 80/100, Test Loss: 0.9827238236154828\n",
      "Epoch 81/100, Loss: 0.9916718176433018\n",
      "Epoch 81/100, Test Loss: 0.9822265080043248\n",
      "Epoch 82/100, Loss: 0.9857471414974758\n",
      "Epoch 82/100, Test Loss: 0.9818662405014038\n",
      "Epoch 83/100, Loss: 0.9862103547368731\n",
      "Epoch 83/100, Test Loss: 0.9821056553295681\n",
      "Epoch 84/100, Loss: 0.9855673483439854\n",
      "Epoch 84/100, Test Loss: 0.9826699325016567\n",
      "Epoch 85/100, Loss: 0.9898197821208409\n",
      "Epoch 85/100, Test Loss: 0.9824968746730259\n",
      "Epoch 86/100, Loss: 0.9917543019567218\n",
      "Epoch 86/100, Test Loss: 0.9818610123225621\n",
      "Epoch 87/100, Loss: 0.9929399149758475\n",
      "Epoch 87/100, Test Loss: 0.9824322121483939\n",
      "Epoch 88/100, Loss: 0.9960563097681318\n",
      "Epoch 88/100, Test Loss: 0.982490496976035\n",
      "Epoch 89/100, Loss: 0.9962476917675563\n",
      "Epoch 89/100, Test Loss: 0.9822297011102948\n",
      "Epoch 90/100, Loss: 0.9803179332188198\n",
      "Epoch 90/100, Test Loss: 0.9826136316571917\n",
      "Epoch 91/100, Loss: 1.0012900999614172\n",
      "Epoch 91/100, Test Loss: 0.982496270111629\n",
      "Epoch 92/100, Loss: 0.9881557907376971\n",
      "Epoch 92/100, Test Loss: 0.9824450101171222\n",
      "Epoch 93/100, Loss: 0.9848062140601022\n",
      "Epoch 93/100, Test Loss: 0.9823916895048959\n",
      "Epoch 94/100, Loss: 0.9888403756277901\n",
      "Epoch 94/100, Test Loss: 0.9828871403421674\n",
      "Epoch 95/100, Loss: 0.9833049007824489\n",
      "Epoch 95/100, Test Loss: 0.9826996581895011\n",
      "Epoch 96/100, Loss: 0.9915702002389091\n",
      "Epoch 96/100, Test Loss: 0.9824458530970982\n",
      "Epoch 97/100, Loss: 0.9878760065351214\n",
      "Epoch 97/100, Test Loss: 0.9824050664901733\n",
      "Epoch 98/100, Loss: 0.9986112543514797\n",
      "Epoch 98/100, Test Loss: 0.9826120734214783\n",
      "Epoch 99/100, Loss: 0.9939671754837036\n",
      "Epoch 99/100, Test Loss: 0.9820419549942017\n",
      "Epoch 100/100, Loss: 0.9881515843527657\n",
      "Epoch 100/100, Test Loss: 0.9817933440208435\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "num_epochs = 100\n",
    "learning_rate = 1e-3\n",
    "batch_size = 32\n",
    "time_steps = 100\n",
    "num_voxels = 200  # Number of voxels (samples)\n",
    "hidden_size = 64  # Hidden size of the RNN\n",
    "num_beta = time_steps  # one beta array per voxel\n",
    "\n",
    "\n",
    "X_train, Y_train = generate_synthetic_data(num_voxels, time_steps)\n",
    "X_test, Y_test = generate_synthetic_data(num_voxels, time_steps)\n",
    "\n",
    "print(X_train.shape)\n",
    "\n",
    "# Create datasets and data loaders\n",
    "dataset_train = DataLoader(\n",
    "    TensorDataset(X_train, Y_train),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    ")\n",
    "\n",
    "dataset_test = DataLoader(\n",
    "    TensorDataset(X_test, Y_test),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    ")\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Model, criterion, optimizer\n",
    "model = RNNModel(input_size=1, hidden_size=hidden_size, num_voxels=num_voxels).to(device)\n",
    "criterion = nn.MSELoss()  # Mean Squared Error for regression task (beta values)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Start training\n",
    "train(model, criterion, dataset_train, dataset_test, optimizer, num_epochs, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML4Science kernel",
   "language": "python",
   "name": "ml4science_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
